inputs (torch.tensor): inputs to train neural network. Size (batch_size, N_in) 
labels (torch.tensor): correct labels. Size (batch_size)



def train(self, inputs, labels, lr=0.001, debug=False):
        outputs =self.forward(inputs) # forward pass
        creloss = loss.cross_entropy_loss(outputs,labels)# calculate loss
        
        accuracy = # calculate accuracy
        
        if debug:
            print('loss: ', creloss)
            print('accuracy: ', accuracy)
        
        dw1, db1, dw2, db2, dw3, db3 = 
        self.weights, self.biases = 
        return creloss, accuracy, outputs 

def predict(self, inputs):
    outputs = forward(inputs)# forward pass
        score, idx = # find max score and its index
        return score, idx
        score (torch.tensor): max score for each class. Size (batch_size)
        idx (torch.tensor): index of most activating neuron. Size (batch_size)  

 def eval(self, inputs, labels, debug=False) 
    Returns:
            loss (float): average cross entropy loss
            accuracy (float): ratio of correctly to uncorrectly classified samples
            outputs (torch.tensor): predictions from neural network. Size (batch_size, N_out)
        """
    outputs =self.forward(inputs) # forward pass
    creloss = loss.cross_entropy_loss(outputs,labels)# calculate loss
    accuracy = # calculate accuracy
    return creloss, accuracy, outputs


 def accuracy(self, outputs, labels):
        """Accuracy of neural network for given outputs and labels.
        
        Calculates ratio of number of correct outputs to total number of examples.

        Args:
            outputs (torch.tensor): outputs predicted by neural network. Size (batch_size, N_out)
            labels (torch.tensor): correct labels. Size (batch_size)
        
        Returns:
            accuracy (float): accuracy score 
        """
        accuracy = 
        return accuracy   

    def forward(self, inputs):
        """Forward pass of neural network

        Calculates score for each class.

        Args:
            inputs (torch.tensor): inputs to train neural network. Size (batch_size, N_in) 

        Returns:
            outputs (torch.tensor): predictions from neural network. Size (batch_size, N_out)
        """
        
        self.cache['z1'] = self.weighted_sum(inputs,self.weights['w1'],self.biases['b1'])
        a1 = activation.sigmoid(self.cache['z1'])
        self.cache['z2'] = self.weighted_sum(a1,self.weights['w2'],self.biases['b2'])
        a2 = activation.sigmoid(cache['z2'])
        self.cache['z3'] = self.weighted_sum(a2,self.weights['w3'],self.biases['b3'])
        outputs = activation.softmax(cache['z3'])
        return outputs

    def weighted_sum(self, X, w, b):
        """Weighted sum at neuron
        
        Args:
            X (torch.tensor): matrix of Size (K, L)
            w (torch.tensor): weight matrix of Size (J, L)
            b (torch.tensor): vector of Size (J)

        Returns:
            result (torch.tensor): w*X + b of Size (K, J)
        """
        mul=torch.mm(w,X)#porduct component
        result=b.add(mul)
        return result

    def backward(self, inputs, labels, outputs):
        """Backward pass of neural network
        
        Changes weights and biases of each layer to reduce loss
        
        Args:
            inputs (torch.tensor): inputs to train neural network. Size (batch_size, N_in) 
            labels (torch.tensor): correct labels. Size (batch_size)
            outputs (torch.tensor): outputs predicted by neural network. Size (batch_size, N_out)
        
        Returns:
            dw1 (torch.tensor): Gradient of loss w.r.t. w1. Size like w1
            db1 (torch.tensor): Gradient of loss w.r.t. b1. Size like b1
            dw2 (torch.tensor): Gradient of loss w.r.t. w2. Size like w2
            db2 (torch.tensor): Gradient of loss w.r.t. b2. Size like b2
            dw3 (torch.tensor): Gradient of loss w.r.t. w3. Size like w3
            db3 (torch.tensor): Gradient of loss w.r.t. b3. Size like b3
        """
        # Calculating derivative of loss w.r.t weighted sum
        dout = 
        d2 = 
        d1 = 
        dw1, db1, dw2, db2, dw3, db3 = # calculate all gradients
        return dw1, db1, dw2, db2, dw3, db3

    def calculate_grad(self, inputs, d1, d2, dout):
        """Calculates gradients for backpropagation
        
        This function is used to calculate gradients like loss w.r.t. weights and biases.

        Args:
            inputs (torch.tensor): inputs to train neural network. Size (batch_size, N_in) 
            dout (torch.tensor): error at output. Size like aout or a3 (or z3)
            d2 (torch.tensor): error at hidden layer 2. Size like a2 (or z2)
            d1 (torch.tensor): error at hidden layer 1. Size like a1 (or z1)

        Returns:
            dw1 (torch.tensor): Gradient of loss w.r.t. w1. Size like w1
            db1 (torch.tensor): Gradient of loss w.r.t. b1. Size like b1
            dw2 (torch.tensor): Gradient of loss w.r.t. w2. Size like w2
            db2 (torch.tensor): Gradient of loss w.r.t. b2. Size like b2
            dw3 (torch.tensor): Gradient of loss w.r.t. w3. Size like w3
            db3 (torch.tensor): Gradient of loss w.r.t. b3. Size like b3
        """
        dw3 = 
        dw2 = 
        dw1 = 

        db1 = 
        db2 = 
        db3 = 
        return dw1, db1, dw2, db2, dw3, db3s